library(s20x)
?mtcars
data(mtcars)
head(mtcars)
# This function calculates a linear model of data through regression + helpful
# analytics and visuals on quality of fit
# df is input data frame, xvar and yvar are the variables used on axes while
# zvar is an optional third variable
myslr = function(df,xvar,yvar,zvar=NULL,...)
{
# f is the formula used to make the linear model. formula and paste
# are necessary because inputs are strings
f = formula(paste(yvar,xvar, sep="~"))
# y.lm is the linear model made with lm()
y.lm = lm(f, data=df)
# this plots the 'Residual vs Fitted' graph
plot(y.lm, which=1)
# this section prints the required information to the console and saves the
# dataframe in a csv for future analysis
print(summary(y.lm))
ciReg(y.lm)
normcheck(y.lm, shapiro.wilk=TRUE)
write.csv2(x=df, file="lmdata.csv")
# the figure is setup here with the linear model graphed along with xvar and
# yvar from df. aes_string is used because of string input to the function.
# A title is also made with theme() is used to center it.
figure = ggplot(df, aes_string(x=xvar, y=yvar),...) +
geom_smooth(method="lm", formula = y~x) +
ggtitle(paste(deparse(substitute(df)), ":", xvar,
"vs", yvar, "with linear regression")) +
theme(plot.title = element_text(hjust = 0.5))
# there are three options for the graph depending on the presence/number
# of level in zvar
# if zvar is not present or intentionally null zvar is not shown and xvar
# and yvar are graphed
if (missing(zvar) || is.null(zvar) == TRUE) {
figure = figure  + geom_point()
}
# if zvar is present evaluate the number of unique values it has
else{
zfactor = factor(eval(parse(text=df[zvar])))
# if the number of levels is greater than 8 discretized colors will not look good,
# use a smooth color range
if(length(levels((zfactor))) > 8) {
figure = figure + geom_point(aes_string(colour=zvar))
}
# if the number of levels is less than 8 use discretized colors.
# also creates additional plot of xvar and yvar grouped by unique values of zvar
else if(length(levels((zfactor))) <= 8) {
figure = figure + geom_point(aes_string(colour=zfactor)) + labs(colour=zvar)
figure2 = ggplot(df, aes_string(x=xvar, y=yvar)) + facet_wrap(zvar) + geom_point()
ggsave(file="bycategory.png", plot=figure2)
print(figure2)
}
}
# save the primary figure and print it to the commandline
ggsave(file="lmplot.png",plot=figure)
print(figure)
# returns y.lm as a linear model is the primary objective of the function
return(y.lm)
}
data(mtcars)
obj = myslr(df=mtcars,xvar="wt",yvar="mpg",zvar="cyl")
# my boot uses bootstrapping to estimate statistics of a larger sample from given, x.
# iter is the number of 'new' data sets created, alpha is the error rate which
# determines confidence level, and fun is the property to be estimated
myboot = function(x, iter=10000, alpha=0.05, fun="var",...)
{
# find the length of x, create a matrix possessing values sample with
# replacement with correct number of values
n=length(x)
y=sample(x,n*iter,replace=TRUE)
mat=matrix(y, nr=n, nc=iter,byrow=TRUE)
# apply is used to apply var to each column of the sampled values
xstat=apply(mat,2,fun)
# relevant statistics, the mean variance, mean deviation,
# and confidence intervals, are printed to the commandline
ci=quantile(xstat,c(alpha/2,1-alpha/2))
print(ci)
print(paste('Average variance=',mean(xstat)))
print(paste('Average deviation=',sqrt(mean(xstat))))
# a histogram showing variance distribution for each set is created with a
# proper title and returned as the output of the function
para=hist(xstat, freq=FALSE,las=1,main=paste("bootstrap estimate of", fun,
"in sample data","\n", "alpha=",
alpha, "\t iterations=", iter),
col="green",...)
para
}
# subset is used to select only the cars with 4 cylinder engines,
# $mpg specifies that mpg is the variable to be tested
data = subset(mtcars,cyl==4)
obj = myboot(x=data$mpg)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(s20x)
?mtcars
data(mtcars)
head(mtcars)
# This function calculates a linear model of data through regression + helpful
# analytics and visuals on quality of fit
# df is input data frame, xvar and yvar are the variables used on axes while
# zvar is an optional third variable
myslr = function(df,xvar,yvar,zvar=NULL,...)
{
# f is the formula used to make the linear model. formula and paste
# are necessary because inputs are strings
f = formula(paste(yvar,xvar, sep="~"))
# y.lm is the linear model made with lm()
y.lm = lm(f, data=df)
# this plots the 'Residual vs Fitted' graph
plot(y.lm, which=1)
# this section prints the required information to the console and saves the
# dataframe in a csv for future analysis
print(summary(y.lm))
ciReg(y.lm)
normcheck(y.lm, shapiro.wilk=TRUE)
write.csv2(x=df, file="lmdata.csv")
# the figure is setup here with the linear model graphed along with xvar and
# yvar from df. aes_string is used because of string input to the function.
# A title is also made with theme() is used to center it.
figure = ggplot(df, aes_string(x=xvar, y=yvar),...) +
geom_smooth(method="lm", formula = y~x) +
ggtitle(paste(deparse(substitute(df)), ":", xvar,
"vs", yvar, "with linear regression")) +
theme(plot.title = element_text(hjust = 0.5))
# there are three options for the graph depending on the presence/number
# of level in zvar
# if zvar is not present or intentionally null zvar is not shown and xvar
# and yvar are graphed
if (missing(zvar) || is.null(zvar) == TRUE) {
figure = figure  + geom_point()
}
# if zvar is present evaluate the number of unique values it has
else{
zfactor = factor(eval(parse(text=df[zvar])))
# if the number of levels is greater than 8 discretized colors will not look good,
# use a smooth color range
if(length(levels((zfactor))) > 8) {
figure = figure + geom_point(aes_string(colour=zvar))
}
# if the number of levels is less than 8 use discretized colors.
# also creates additional plot of xvar and yvar grouped by unique values of zvar
else if(length(levels((zfactor))) <= 8) {
figure = figure + geom_point(aes_string(colour=zfactor)) + labs(colour=zvar)
figure2 = ggplot(df, aes_string(x=xvar, y=yvar)) + facet_wrap(zvar) + geom_point()
ggsave(file="bycategory.png", plot=figure2)
print(figure2)
}
}
# save the primary figure and print it to the commandline
ggsave(file="lmplot.png",plot=figure)
print(figure)
# returns y.lm as a linear model is the primary objective of the function
return(y.lm)
}
data(mtcars)
obj = myslr(df=mtcars,xvar="wt",yvar="mpg",zvar="cyl")
# my boot uses bootstrapping to estimate statistics of a larger sample from given, x.
# iter is the number of 'new' data sets created, alpha is the error rate which
# determines confidence level, and fun is the property to be estimated
myboot = function(x, iter=10000, alpha=0.05, fun="var",...)
{
# find the length of x, create a matrix possessing values sample with
# replacement with correct number of values
n=length(x)
y=sample(x,n*iter,replace=TRUE)
mat=matrix(y, nr=n, nc=iter,byrow=TRUE)
# apply is used to apply var to each column of the sampled values
xstat=apply(mat,2,fun)
# relevant statistics, the mean variance, mean deviation,
# and confidence intervals, are printed to the commandline
ci=quantile(xstat,c(alpha/2,1-alpha/2))
print(ci)
print(paste('Average variance=',mean(xstat)))
print(paste('Average deviation=',sqrt(mean(xstat))))
# a histogram showing variance distribution for each set is created with a
# proper title and returned as the output of the function
para=hist(xstat, freq=FALSE,las=1,main=paste("bootstrap estimate of", fun,
"in sample data","\n", "alpha=",
alpha, "\t iterations=", iter),
col="green",...)
para
}
# subset is used to select only the cars with 4 cylinder engines,
# $mpg specifies that mpg is the variable to be tested
data = subset(mtcars,cyl==4)
obj = myboot(x=data$mpg)
install.packages(c("digest", "glue", "stringi", "tinytex", "uuid", "xfun"))
install.packages(c("digest", "glue", "stringi", "tinytex", "uuid", "xfun"))
install.packages(c("digest", "glue", "stringi", "tinytex", "uuid", "xfun"))
install.packages(c("digest", "glue", "stringi", "tinytex", "uuid", "xfun"))
install.packages(c("digest", "glue", "stringi", "tinytex", "uuid", "xfun"))
install.packages(c("digest", "glue", "stringi", "tinytex", "uuid", "xfun"))
install.packages(c("digest", "glue", "stringi", "tinytex", "uuid", "xfun"))
install.packages(c("digest", "glue", "stringi", "tinytex", "uuid", "xfun"))
install.packages(c("digest", "glue", "stringi", "tinytex", "uuid", "xfun"))
install.packages(c("digest", "glue", "stringi", "tinytex", "uuid", "xfun"))
install.packages(c("digest", "glue", "stringi", "tinytex", "uuid", "xfun"))
install.packages(c("digest", "glue", "stringi", "tinytex", "uuid", "xfun"))
install.packages(c("digest", "glue", "stringi", "tinytex", "uuid", "xfun"))
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(s20x)
?mtcars
data(mtcars)
head(mtcars)
# This function calculates a linear model of data through regression + helpful
# analytics and visuals on quality of fit
# df is input data frame, xvar and yvar are the variables used on axes while
# zvar is an optional third variable
myslr = function(df,xvar,yvar,zvar=NULL,...)
{
# f is the formula used to make the linear model. formula and paste
# are necessary because inputs are strings
f = formula(paste(yvar,xvar, sep="~"))
# y.lm is the linear model made with lm()
y.lm = lm(f, data=df)
# this plots the 'Residual vs Fitted' graph
plot(y.lm, which=1)
# this section prints the required information to the console and saves the
# dataframe in a csv for future analysis
print(summary(y.lm))
ciReg(y.lm)
normcheck(y.lm, shapiro.wilk=TRUE)
write.csv2(x=df, file="lmdata.csv")
# the figure is setup here with the linear model graphed along with xvar and
# yvar from df. aes_string is used because of string input to the function.
# A title is also made with theme() is used to center it.
figure = ggplot(df, aes_string(x=xvar, y=yvar),...) +
geom_smooth(method="lm", formula = y~x) +
ggtitle(paste(deparse(substitute(df)), ":", xvar,
"vs", yvar, "with linear regression")) +
theme(plot.title = element_text(hjust = 0.5))
# there are three options for the graph depending on the presence/number
# of level in zvar
# if zvar is not present or intentionally null zvar is not shown and xvar
# and yvar are graphed
if (missing(zvar) || is.null(zvar) == TRUE) {
figure = figure  + geom_point()
}
# if zvar is present evaluate the number of unique values it has
else{
zfactor = factor(eval(parse(text=df[zvar])))
# if the number of levels is greater than 8 discretized colors will not look good,
# use a smooth color range
if(length(levels((zfactor))) > 8) {
figure = figure + geom_point(aes_string(colour=zvar))
}
# if the number of levels is less than 8 use discretized colors.
# also creates additional plot of xvar and yvar grouped by unique values of zvar
else if(length(levels((zfactor))) <= 8) {
figure = figure + geom_point(aes_string(colour=zfactor)) + labs(colour=zvar)
figure2 = ggplot(df, aes_string(x=xvar, y=yvar)) + facet_wrap(zvar) + geom_point()
ggsave(file="bycategory.png", plot=figure2)
print(figure2)
}
}
# save the primary figure and print it to the commandline
ggsave(file="lmplot.png",plot=figure)
print(figure)
# returns y.lm as a linear model is the primary objective of the function
return(y.lm)
}
data(mtcars)
obj = myslr(df=mtcars,xvar="wt",yvar="mpg",zvar="cyl")
# my boot uses bootstrapping to estimate statistics of a larger sample from given, x.
# iter is the number of 'new' data sets created, alpha is the error rate which
# determines confidence level, and fun is the property to be estimated
myboot = function(x, iter=10000, alpha=0.05, fun="var",...)
{
# find the length of x, create a matrix possessing values sample with
# replacement with correct number of values
n=length(x)
y=sample(x,n*iter,replace=TRUE)
mat=matrix(y, nr=n, nc=iter,byrow=TRUE)
# apply is used to apply var to each column of the sampled values
xstat=apply(mat,2,fun)
# relevant statistics, the mean variance, mean deviation,
# and confidence intervals, are printed to the commandline
ci=quantile(xstat,c(alpha/2,1-alpha/2))
print(ci)
print(paste('Average variance=',mean(xstat)))
print(paste('Average deviation=',sqrt(mean(xstat))))
# a histogram showing variance distribution for each set is created with a
# proper title and returned as the output of the function
para=hist(xstat, freq=FALSE,las=1,main=paste("bootstrap estimate of", fun,
"in sample data","\n", "alpha=",
alpha, "\t iterations=", iter),
col="green",...)
para
}
# subset is used to select only the cars with 4 cylinder engines,
# $mpg specifies that mpg is the variable to be tested
data = subset(mtcars,cyl==4)
obj = myboot(x=data$mpg)
knitr::opts_chunk$set(echo = TRUE)
?mtcars
data(mtcars)
head(mtcars)
# This function calculates a linear model of data through regression + helpful
# analytics and visuals on quality of fit
# df is input data frame, xvar and yvar are the variables used on axes while
# zvar is an optional third variable
myslr = function(df,xvar,yvar,zvar=NULL,...)
{
# f is the formula used to make the linear model. formula and paste
# are necessary because inputs are strings
f = formula(paste(yvar,xvar, sep="~"))
# y.lm is the linear model made with lm()
y.lm = lm(f, data=df)
# this plots the 'Residual vs Fitted' graph
plot(y.lm, which=1)
# this section prints the required information to the console and saves the
# dataframe in a csv for future analysis
print(summary(y.lm))
ciReg(y.lm)
normcheck(y.lm, shapiro.wilk=TRUE)
write.csv2(x=df, file="lmdata.csv")
# the figure is setup here with the linear model graphed along with xvar and
# yvar from df. aes_string is used because of string input to the function.
# A title is also made with theme() is used to center it.
figure = ggplot(df, aes_string(x=xvar, y=yvar),...) +
geom_smooth(method="lm", formula = y~x) +
ggtitle(paste(deparse(substitute(df)), ":", xvar,
"vs", yvar, "with linear regression")) +
theme(plot.title = element_text(hjust = 0.5))
# there are three options for the graph depending on the presence/number
# of level in zvar
# if zvar is not present or intentionally null zvar is not shown and xvar
# and yvar are graphed
if (missing(zvar) || is.null(zvar) == TRUE) {
figure = figure  + geom_point()
}
# if zvar is present evaluate the number of unique values it has
else{
zfactor = factor(eval(parse(text=df[zvar])))
# if the number of levels is greater than 8 discretized colors will not look good,
# use a smooth color range
if(length(levels((zfactor))) > 8) {
figure = figure + geom_point(aes_string(colour=zvar))
}
# if the number of levels is less than 8 use discretized colors.
# also creates additional plot of xvar and yvar grouped by unique values of zvar
else if(length(levels((zfactor))) <= 8) {
figure = figure + geom_point(aes_string(colour=zfactor)) + labs(colour=zvar)
figure2 = ggplot(df, aes_string(x=xvar, y=yvar)) + facet_wrap(zvar) + geom_point()
ggsave(file="bycategory.png", plot=figure2)
print(figure2)
}
}
# save the primary figure and print it to the commandline
ggsave(file="lmplot.png",plot=figure)
print(figure)
# returns y.lm as a linear model is the primary objective of the function
return(y.lm)
}
data(mtcars)
obj = myslr(df=mtcars,xvar="wt",yvar="mpg",zvar="cyl")
# my boot uses bootstrapping to estimate statistics of a larger sample from given, x.
# iter is the number of 'new' data sets created, alpha is the error rate which
# determines confidence level, and fun is the property to be estimated
myboot = function(x, iter=10000, alpha=0.05, fun="var",...)
{
# find the length of x, create a matrix possessing values sample with
# replacement with correct number of values
n=length(x)
y=sample(x,n*iter,replace=TRUE)
mat=matrix(y, nr=n, nc=iter,byrow=TRUE)
# apply is used to apply var to each column of the sampled values
xstat=apply(mat,2,fun)
# relevant statistics, the mean variance, mean deviation,
# and confidence intervals, are printed to the commandline
ci=quantile(xstat,c(alpha/2,1-alpha/2))
print(ci)
print(paste('Average variance=',mean(xstat)))
print(paste('Average deviation=',sqrt(mean(xstat))))
# a histogram showing variance distribution for each set is created with a
# proper title and returned as the output of the function
para=hist(xstat, freq=FALSE,las=1,main=paste("bootstrap estimate of", fun,
"in sample data","\n", "alpha=",
alpha, "\t iterations=", iter),
col="green",...)
para
}
# subset is used to select only the cars with 4 cylinder engines,
# $mpg specifies that mpg is the variable to be tested
data = subset(mtcars,cyl==4)
obj = myboot(x=data$mpg)
install.packages("installr")
install.packages('rtools')
install.packages('installr')
install.packages(c("ggplot2", "tidyverse", "dplyr", "data.table", "prettydoc", "readxl", "stringr", "purrr", "s20x", "gpairs", "boot", "shiny", "MCMCpack", "ggmcmc", "coda", "plotly", "roxygen2", "devtools", "usethis","rstan","shinystan"), type = "binary", quiet = TRUE, dependencies = TRUE)
devtools::install_github("MATHSTATSOU/Intro2R",dependencies = "Imports", upgrade = TRUE, force = TRUE,build_vignettes = TRUE)
help(package = Intro2R)
setwd("C:/Users/Admin/Desktop/Notes 2021/R")
#Task 1
getwd()
setwd("C:/Users/Admin/Desktop/Applied Statistical Methods/Lab 1")
#Task 1
getwd()
ddt = read.csv("DDT.csv")
# First six lines
head(ddt)
getwd()
ddt = read.csv("DDT.csv")
getwd()
ddt = read.csv("DDT.csv")
head(ddt)
class(ddt)
sapply(ddt,class)
getwd()
ddt = read.csv("DDT.csv")
head(ddt)
class(ddt)
sapply(ddt,class)
ddt = read.csv("DDT.csv")
head(ddt)
getwd()
ddt = read.csv("DDT.csv")
head(ddt)
class(ddt)
sapply(ddt,class)
levels(ddt$species)
levels(ddt$SPECIES)
levels(ddt)
levels(ddt$SPECIES)
View(ddt)
levels(ddt$SPECIES)
getwd()
ddt = read.csv("DDT.csv", stringsAsFactors = True)
getwd()
ddt = read.csv("DDT.csv", stringsAsFactors = TRUE)
head(ddt)
class(ddt)
sapply(ddt,class)
levels(ddt$SPECIES)
getwd()
ddt = read.csv("DDT.csv")
head(ddt)
class(ddt)
sapply(ddt,class)
levels(ddt$SPECIES)
getwd()
ddt = read.csv("DDT.csv")
head(ddt)
class(ddt)
sapply(ddt,class)
unique(ddt$SPECIES)
sum(unique(ddt$SPECIES))
count(unique(ddt$SPECIES))
length(unique(ddt$SPECIES))
unique(ddt$SPECIES)
?subset
subset(ddt, WEIGHT > 800, select = c(species, lmbass, weight))
subset(ddt, WEIGHT > 800, select = c(SPECIES, LMBASS, weight))
subset(ddt, WEIGHT > 800, species == LMBASS, select = c(SPECIES, LMBASS, weight))
subset(ddt, WEIGHT > 800, species == 'LMBASS', select = c(SPECIES, LMBASS, weight))
subset(ddt, WEIGHT > 800, SPECIES == 'LMBASS', select = c(SPECIES, LMBASS, weight))
subset(ddt, WEIGHT > 800, SPECIES == 'LMBASS')
subset(ddt, WEIGHT > 800 && SPECIES == 'LMBASS')
subset(ddt, WEIGHT > 800 & SPECIES == 'LMBASS')
subset(ddt, WEIGHT > 800 & SPECIES == 'LMBASS')
subset(ddt, ddt > 4.0 & RIVER == 'SCM')
subset(ddt, WEIGHT > 800 & SPECIES == 'LMBASS')
subset(ddt, DDT > 4.0 & RIVER == 'SCM')
summary(ddt$LENGTH)
mean(ddt$LENGTH)
sd(ddt$LENGTH)
mean(ddt$LENGTH)
sd(ddt$LENGTH)
plot(ddt$WEIGHT, ddt$LENGTH)
mean(ddt$LENGTH)
sd(ddt$LENGTH)
plot(ddt$WEIGHT, ddt$LENGTH)
v = 1:20
v[20]
table(ddt$RIVER)
View(ddt)
table(ddt$RIVER)
barplot(ddt$RIVER)
table(ddt$RIVER)
barplot(ddt$RIVER)
table(ddt$RIVER)
barplot(table(ddt$RIVER))
table(ddt$RIVER)
barplot(table(ddt$RIVER), col = rainbow)
table(ddt$RIVER)
barplot((table(ddt$RIVER), col = rainbow(1)))
table(ddt$RIVER)
barplot(table(ddt$RIVER), col = rainbow(1))
table(ddt$RIVER)
barplot(table(ddt$RIVER), col = rainbow())
table(ddt$RIVER)
barplot(table(ddt$RIVER), col = rainbow(1:5))
table(ddt$RIVER)
barplot(table(ddt$RIVER), col = rainbow(1:4))
table(ddt$RIVER)
barplot(table(ddt$RIVER), col = (red, blue, green, black))
table(ddt$RIVER)
barplot(table(ddt$RIVER), col = c(red, blue, green, black))
table(ddt$RIVER)
barplot(table(ddt$RIVER), col = c('red', 'blue', 'green', 'black'))
table(ddt$RIVER)
barplot(table(ddt$RIVER), col = c('red', 'blue', 'green', 'orange'))
table(ddt$RIVER, ddt$SPECIES)
barplot(ddt$RIVER, ddt$SPECIES, col = c('red', 'blue', 'green', 'orange'))
table(ddt$RIVER, ddt$SPECIES)
table(ddt$RIVER, ddt$SPECIES)
barplot(ddt$RIVER, ddt$SPECIES)
